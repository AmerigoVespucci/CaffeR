Log file created at: 2015/08/12 10:49:41
Running on machine: AbbaHostedByChayim
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0812 10:49:41.650602 16199 caffe.cpp:117] Use CPU.
I0812 10:49:41.650863 16199 caffe.cpp:121] Starting Optimization
I0812 10:49:41.650918 16199 solver.cpp:32] Initializing solver from parameters: 
train_net: "/home/abba/caffe/examples/simple/train.prototxt"
test_net: "/home/abba/caffe/examples/simple/train.prototxt"
test_iter: 100
test_interval: 1000
base_lr: 0.0001
display: 500
max_iter: 10000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 50000
snapshot: 2000
snapshot_prefix: "/home/abba/caffe/examples/simple/simple"
solver_mode: CPU
I0812 10:49:41.650985 16199 solver.cpp:61] Creating training net from train_net file: /home/abba/caffe/examples/simple/train.prototxt
I0812 10:49:41.651378 16199 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0812 10:49:41.651408 16199 net.cpp:287] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0812 10:49:41.651484 16199 net.cpp:42] Initializing net from parameters: 
name: "LogisticRegressionNet"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/home/abba/caffe/examples/simple/train_list.txt"
    batch_size: 5
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "data"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label"
  top: "loss"
}
I0812 10:49:41.651674 16199 net.cpp:67] Memory required for data: 0
I0812 10:49:41.651700 16199 layer_factory.hpp:74] Creating layer data
I0812 10:49:41.651717 16199 net.cpp:90] Creating Layer data
I0812 10:49:41.651731 16199 net.cpp:368] data -> data
I0812 10:49:41.651753 16199 net.cpp:368] data -> label
I0812 10:49:41.651768 16199 net.cpp:120] Setting up data
I0812 10:49:41.651779 16199 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/abba/caffe/examples/simple/train_list.txt
I0812 10:49:41.651826 16199 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I0812 10:49:41.651839 16199 hdf5_data_layer.cpp:29] Loading HDF5 file: /home/abba/caffe/examples/simple/train.h5
I0812 10:49:41.652858 16199 hdf5_data_layer.cpp:68] Successully loaded 100 rows
I0812 10:49:41.652906 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.652920 16199 net.cpp:127] Top shape: 5 1 (5)
I0812 10:49:41.652928 16199 net.cpp:133] Memory required for data: 60
I0812 10:49:41.652940 16199 layer_factory.hpp:74] Creating layer fc1
I0812 10:49:41.652962 16199 net.cpp:90] Creating Layer fc1
I0812 10:49:41.652973 16199 net.cpp:410] fc1 <- data
I0812 10:49:41.652995 16199 net.cpp:368] fc1 -> fc1
I0812 10:49:41.653013 16199 net.cpp:120] Setting up fc1
I0812 10:49:41.653398 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.653414 16199 net.cpp:133] Memory required for data: 100
I0812 10:49:41.653431 16199 layer_factory.hpp:74] Creating layer loss
I0812 10:49:41.653456 16199 net.cpp:90] Creating Layer loss
I0812 10:49:41.653466 16199 net.cpp:410] loss <- fc1
I0812 10:49:41.653478 16199 net.cpp:410] loss <- label
I0812 10:49:41.653492 16199 net.cpp:368] loss -> loss
I0812 10:49:41.653506 16199 net.cpp:120] Setting up loss
I0812 10:49:41.653520 16199 layer_factory.hpp:74] Creating layer loss
I0812 10:49:41.653548 16199 net.cpp:127] Top shape: (1)
I0812 10:49:41.653556 16199 net.cpp:129]     with loss weight 1
I0812 10:49:41.653578 16199 net.cpp:133] Memory required for data: 104
I0812 10:49:41.653587 16199 net.cpp:192] loss needs backward computation.
I0812 10:49:41.653597 16199 net.cpp:192] fc1 needs backward computation.
I0812 10:49:41.653607 16199 net.cpp:194] data does not need backward computation.
I0812 10:49:41.653616 16199 net.cpp:235] This network produces output loss
I0812 10:49:41.653630 16199 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0812 10:49:41.653640 16199 net.cpp:247] Network initialization done.
I0812 10:49:41.653668 16199 net.cpp:248] Memory required for data: 104
I0812 10:49:41.653831 16199 solver.cpp:154] Creating test net (#0) specified by test_net file: /home/abba/caffe/examples/simple/train.prototxt
I0812 10:49:41.653861 16199 net.cpp:287] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0812 10:49:41.653921 16199 net.cpp:42] Initializing net from parameters: 
name: "LogisticRegressionNet"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/home/abba/caffe/examples/simple/test_list.txt"
    batch_size: 5
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "data"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I0812 10:49:41.654141 16199 net.cpp:67] Memory required for data: 0
I0812 10:49:41.654166 16199 layer_factory.hpp:74] Creating layer data
I0812 10:49:41.654184 16199 net.cpp:90] Creating Layer data
I0812 10:49:41.654196 16199 net.cpp:368] data -> data
I0812 10:49:41.654216 16199 net.cpp:368] data -> label
I0812 10:49:41.654230 16199 net.cpp:120] Setting up data
I0812 10:49:41.654242 16199 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: /home/abba/caffe/examples/simple/test_list.txt
I0812 10:49:41.654266 16199 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I0812 10:49:41.654278 16199 hdf5_data_layer.cpp:29] Loading HDF5 file: /home/abba/caffe/examples/simple/test.h5
I0812 10:49:41.654665 16199 hdf5_data_layer.cpp:68] Successully loaded 30 rows
I0812 10:49:41.654692 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.654705 16199 net.cpp:127] Top shape: 5 1 (5)
I0812 10:49:41.654712 16199 net.cpp:133] Memory required for data: 60
I0812 10:49:41.654722 16199 layer_factory.hpp:74] Creating layer label_data_1_split
I0812 10:49:41.654739 16199 net.cpp:90] Creating Layer label_data_1_split
I0812 10:49:41.654757 16199 net.cpp:410] label_data_1_split <- label
I0812 10:49:41.654772 16199 net.cpp:368] label_data_1_split -> label_data_1_split_0
I0812 10:49:41.654789 16199 net.cpp:368] label_data_1_split -> label_data_1_split_1
I0812 10:49:41.654803 16199 net.cpp:120] Setting up label_data_1_split
I0812 10:49:41.654817 16199 net.cpp:127] Top shape: 5 1 (5)
I0812 10:49:41.654827 16199 net.cpp:127] Top shape: 5 1 (5)
I0812 10:49:41.654834 16199 net.cpp:133] Memory required for data: 100
I0812 10:49:41.654844 16199 layer_factory.hpp:74] Creating layer fc1
I0812 10:49:41.654867 16199 net.cpp:90] Creating Layer fc1
I0812 10:49:41.654878 16199 net.cpp:410] fc1 <- data
I0812 10:49:41.654894 16199 net.cpp:368] fc1 -> fc1
I0812 10:49:41.654911 16199 net.cpp:120] Setting up fc1
I0812 10:49:41.654939 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.654950 16199 net.cpp:133] Memory required for data: 140
I0812 10:49:41.654976 16199 layer_factory.hpp:74] Creating layer fc1_fc1_0_split
I0812 10:49:41.654989 16199 net.cpp:90] Creating Layer fc1_fc1_0_split
I0812 10:49:41.654999 16199 net.cpp:410] fc1_fc1_0_split <- fc1
I0812 10:49:41.655011 16199 net.cpp:368] fc1_fc1_0_split -> fc1_fc1_0_split_0
I0812 10:49:41.655025 16199 net.cpp:368] fc1_fc1_0_split -> fc1_fc1_0_split_1
I0812 10:49:41.655040 16199 net.cpp:120] Setting up fc1_fc1_0_split
I0812 10:49:41.655053 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.655071 16199 net.cpp:127] Top shape: 5 2 (10)
I0812 10:49:41.655081 16199 net.cpp:133] Memory required for data: 220
I0812 10:49:41.655089 16199 layer_factory.hpp:74] Creating layer loss
I0812 10:49:41.655102 16199 net.cpp:90] Creating Layer loss
I0812 10:49:41.655112 16199 net.cpp:410] loss <- fc1_fc1_0_split_0
I0812 10:49:41.655153 16199 net.cpp:410] loss <- label_data_1_split_0
I0812 10:49:41.655169 16199 net.cpp:368] loss -> loss
I0812 10:49:41.655184 16199 net.cpp:120] Setting up loss
I0812 10:49:41.655194 16199 layer_factory.hpp:74] Creating layer loss
I0812 10:49:41.655218 16199 net.cpp:127] Top shape: (1)
I0812 10:49:41.655227 16199 net.cpp:129]     with loss weight 1
I0812 10:49:41.655256 16199 net.cpp:133] Memory required for data: 224
I0812 10:49:41.655266 16199 layer_factory.hpp:74] Creating layer accuracy
I0812 10:49:41.655278 16199 net.cpp:90] Creating Layer accuracy
I0812 10:49:41.655287 16199 net.cpp:410] accuracy <- fc1_fc1_0_split_1
I0812 10:49:41.655299 16199 net.cpp:410] accuracy <- label_data_1_split_1
I0812 10:49:41.655311 16199 net.cpp:368] accuracy -> accuracy
I0812 10:49:41.655326 16199 net.cpp:120] Setting up accuracy
I0812 10:49:41.655339 16199 net.cpp:127] Top shape: (1)
I0812 10:49:41.655347 16199 net.cpp:133] Memory required for data: 228
I0812 10:49:41.655355 16199 net.cpp:194] accuracy does not need backward computation.
I0812 10:49:41.655365 16199 net.cpp:192] loss needs backward computation.
I0812 10:49:41.655375 16199 net.cpp:192] fc1_fc1_0_split needs backward computation.
I0812 10:49:41.655383 16199 net.cpp:192] fc1 needs backward computation.
I0812 10:49:41.655392 16199 net.cpp:194] label_data_1_split does not need backward computation.
I0812 10:49:41.655402 16199 net.cpp:194] data does not need backward computation.
I0812 10:49:41.655410 16199 net.cpp:235] This network produces output accuracy
I0812 10:49:41.655418 16199 net.cpp:235] This network produces output loss
I0812 10:49:41.655434 16199 net.cpp:482] Collecting Learning Rate and Weight Decay.
I0812 10:49:41.655444 16199 net.cpp:247] Network initialization done.
I0812 10:49:41.655452 16199 net.cpp:248] Memory required for data: 228
I0812 10:49:41.655479 16199 solver.cpp:42] Solver scaffolding done.
I0812 10:49:41.655498 16199 solver.cpp:250] Solving LogisticRegressionNet
I0812 10:49:41.655505 16199 solver.cpp:251] Learning Rate Policy: step
I0812 10:49:41.655514 16199 solver.cpp:294] Iteration 0, Testing net (#0)
I0812 10:49:41.655530 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.655539 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.655550 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.657058 16199 solver.cpp:343]     Test net output #0: accuracy = 0.566
I0812 10:49:41.657101 16199 solver.cpp:343]     Test net output #1: loss = 0.988253 (* 1 = 0.988253 loss)
I0812 10:49:41.657156 16199 solver.cpp:214] Iteration 0, loss = 0.812613
I0812 10:49:41.657174 16199 solver.cpp:229]     Train net output #0: loss = 0.812613 (* 1 = 0.812613 loss)
I0812 10:49:41.657189 16199 solver.cpp:486] Iteration 0, lr = 0.0001
I0812 10:49:41.662328 16199 solver.cpp:214] Iteration 500, loss = 0.680443
I0812 10:49:41.662369 16199 solver.cpp:229]     Train net output #0: loss = 0.680443 (* 1 = 0.680443 loss)
I0812 10:49:41.662382 16199 solver.cpp:486] Iteration 500, lr = 0.0001
I0812 10:49:41.667453 16199 solver.cpp:294] Iteration 1000, Testing net (#0)
I0812 10:49:41.667490 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.667500 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.667510 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.668728 16199 solver.cpp:343]     Test net output #0: accuracy = 0.472
I0812 10:49:41.668756 16199 solver.cpp:343]     Test net output #1: loss = 0.799378 (* 1 = 0.799378 loss)
I0812 10:49:41.668781 16199 solver.cpp:214] Iteration 1000, loss = 0.618377
I0812 10:49:41.668794 16199 solver.cpp:229]     Train net output #0: loss = 0.618377 (* 1 = 0.618377 loss)
I0812 10:49:41.668805 16199 solver.cpp:486] Iteration 1000, lr = 0.0001
I0812 10:49:41.674243 16199 solver.cpp:214] Iteration 1500, loss = 0.590874
I0812 10:49:41.674363 16199 solver.cpp:229]     Train net output #0: loss = 0.590874 (* 1 = 0.590874 loss)
I0812 10:49:41.674379 16199 solver.cpp:486] Iteration 1500, lr = 0.0001
I0812 10:49:41.679497 16199 net.cpp:763] Serializing 3 layers
I0812 10:49:41.679585 16199 solver.cpp:361] Snapshotting to /home/abba/caffe/examples/simple/simple_iter_2000.caffemodel
I0812 10:49:41.679769 16199 solver.cpp:369] Snapshotting solver state to /home/abba/caffe/examples/simple/simple_iter_2000.solverstate
I0812 10:49:41.679899 16199 solver.cpp:294] Iteration 2000, Testing net (#0)
I0812 10:49:41.679926 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.679937 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.679947 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.681195 16199 solver.cpp:343]     Test net output #0: accuracy = 0.394
I0812 10:49:41.681231 16199 solver.cpp:343]     Test net output #1: loss = 0.760197 (* 1 = 0.760197 loss)
I0812 10:49:41.681262 16199 solver.cpp:214] Iteration 2000, loss = 0.576631
I0812 10:49:41.681277 16199 solver.cpp:229]     Train net output #0: loss = 0.576631 (* 1 = 0.576631 loss)
I0812 10:49:41.681289 16199 solver.cpp:486] Iteration 2000, lr = 0.0001
I0812 10:49:41.686372 16199 solver.cpp:214] Iteration 2500, loss = 0.567062
I0812 10:49:41.686416 16199 solver.cpp:229]     Train net output #0: loss = 0.567062 (* 1 = 0.567062 loss)
I0812 10:49:41.686429 16199 solver.cpp:486] Iteration 2500, lr = 0.0001
I0812 10:49:41.692215 16199 solver.cpp:294] Iteration 3000, Testing net (#0)
I0812 10:49:41.692267 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.692276 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.692286 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.693732 16199 solver.cpp:343]     Test net output #0: accuracy = 0.434
I0812 10:49:41.693766 16199 solver.cpp:343]     Test net output #1: loss = 0.728924 (* 1 = 0.728924 loss)
I0812 10:49:41.693804 16199 solver.cpp:214] Iteration 3000, loss = 0.559152
I0812 10:49:41.693819 16199 solver.cpp:229]     Train net output #0: loss = 0.559152 (* 1 = 0.559152 loss)
I0812 10:49:41.693831 16199 solver.cpp:486] Iteration 3000, lr = 0.0001
I0812 10:49:41.698961 16199 solver.cpp:214] Iteration 3500, loss = 0.551905
I0812 10:49:41.698986 16199 solver.cpp:229]     Train net output #0: loss = 0.551905 (* 1 = 0.551905 loss)
I0812 10:49:41.698997 16199 solver.cpp:486] Iteration 3500, lr = 0.0001
I0812 10:49:41.703994 16199 net.cpp:763] Serializing 3 layers
I0812 10:49:41.704035 16199 solver.cpp:361] Snapshotting to /home/abba/caffe/examples/simple/simple_iter_4000.caffemodel
I0812 10:49:41.704206 16199 solver.cpp:369] Snapshotting solver state to /home/abba/caffe/examples/simple/simple_iter_4000.solverstate
I0812 10:49:41.704331 16199 solver.cpp:294] Iteration 4000, Testing net (#0)
I0812 10:49:41.704356 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.704365 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.704375 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.705662 16199 solver.cpp:343]     Test net output #0: accuracy = 0.506
I0812 10:49:41.705754 16199 solver.cpp:343]     Test net output #1: loss = 0.700686 (* 1 = 0.700686 loss)
I0812 10:49:41.705806 16199 solver.cpp:214] Iteration 4000, loss = 0.545005
I0812 10:49:41.705824 16199 solver.cpp:229]     Train net output #0: loss = 0.545005 (* 1 = 0.545005 loss)
I0812 10:49:41.705839 16199 solver.cpp:486] Iteration 4000, lr = 0.0001
I0812 10:49:41.711038 16199 solver.cpp:214] Iteration 4500, loss = 0.538351
I0812 10:49:41.711072 16199 solver.cpp:229]     Train net output #0: loss = 0.538351 (* 1 = 0.538351 loss)
I0812 10:49:41.711083 16199 solver.cpp:486] Iteration 4500, lr = 0.0001
I0812 10:49:41.716094 16199 solver.cpp:294] Iteration 5000, Testing net (#0)
I0812 10:49:41.716110 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.716130 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.716140 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.717450 16199 solver.cpp:343]     Test net output #0: accuracy = 0.494
I0812 10:49:41.717470 16199 solver.cpp:343]     Test net output #1: loss = 0.685651 (* 1 = 0.685651 loss)
I0812 10:49:41.717494 16199 solver.cpp:214] Iteration 5000, loss = 0.53191
I0812 10:49:41.717507 16199 solver.cpp:229]     Train net output #0: loss = 0.53191 (* 1 = 0.53191 loss)
I0812 10:49:41.717542 16199 solver.cpp:486] Iteration 5000, lr = 0.0001
I0812 10:49:41.722723 16199 solver.cpp:214] Iteration 5500, loss = 0.52567
I0812 10:49:41.722815 16199 solver.cpp:229]     Train net output #0: loss = 0.52567 (* 1 = 0.52567 loss)
I0812 10:49:41.722831 16199 solver.cpp:486] Iteration 5500, lr = 0.0001
I0812 10:49:41.728008 16199 net.cpp:763] Serializing 3 layers
I0812 10:49:41.728049 16199 solver.cpp:361] Snapshotting to /home/abba/caffe/examples/simple/simple_iter_6000.caffemodel
I0812 10:49:41.728221 16199 solver.cpp:369] Snapshotting solver state to /home/abba/caffe/examples/simple/simple_iter_6000.solverstate
I0812 10:49:41.728345 16199 solver.cpp:294] Iteration 6000, Testing net (#0)
I0812 10:49:41.728370 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.728380 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.728390 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.729677 16199 solver.cpp:343]     Test net output #0: accuracy = 0.5
I0812 10:49:41.729699 16199 solver.cpp:343]     Test net output #1: loss = 0.66089 (* 1 = 0.66089 loss)
I0812 10:49:41.729735 16199 solver.cpp:214] Iteration 6000, loss = 0.519623
I0812 10:49:41.729750 16199 solver.cpp:229]     Train net output #0: loss = 0.519623 (* 1 = 0.519623 loss)
I0812 10:49:41.729761 16199 solver.cpp:486] Iteration 6000, lr = 0.0001
I0812 10:49:41.734877 16199 solver.cpp:214] Iteration 6500, loss = 0.513764
I0812 10:49:41.734910 16199 solver.cpp:229]     Train net output #0: loss = 0.513764 (* 1 = 0.513764 loss)
I0812 10:49:41.734921 16199 solver.cpp:486] Iteration 6500, lr = 0.0001
I0812 10:49:41.740195 16199 solver.cpp:294] Iteration 7000, Testing net (#0)
I0812 10:49:41.740232 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.740242 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.740250 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.741533 16199 solver.cpp:343]     Test net output #0: accuracy = 0.54
I0812 10:49:41.741561 16199 solver.cpp:343]     Test net output #1: loss = 0.636951 (* 1 = 0.636951 loss)
I0812 10:49:41.741587 16199 solver.cpp:214] Iteration 7000, loss = 0.508087
I0812 10:49:41.741602 16199 solver.cpp:229]     Train net output #0: loss = 0.508087 (* 1 = 0.508087 loss)
I0812 10:49:41.741613 16199 solver.cpp:486] Iteration 7000, lr = 0.0001
I0812 10:49:41.746634 16199 solver.cpp:214] Iteration 7500, loss = 0.502585
I0812 10:49:41.746673 16199 solver.cpp:229]     Train net output #0: loss = 0.502585 (* 1 = 0.502585 loss)
I0812 10:49:41.746683 16199 solver.cpp:486] Iteration 7500, lr = 0.0001
I0812 10:49:41.751886 16199 net.cpp:763] Serializing 3 layers
I0812 10:49:41.751931 16199 solver.cpp:361] Snapshotting to /home/abba/caffe/examples/simple/simple_iter_8000.caffemodel
I0812 10:49:41.752104 16199 solver.cpp:369] Snapshotting solver state to /home/abba/caffe/examples/simple/simple_iter_8000.solverstate
I0812 10:49:41.752224 16199 solver.cpp:294] Iteration 8000, Testing net (#0)
I0812 10:49:41.752250 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.752259 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.752269 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.753588 16199 solver.cpp:343]     Test net output #0: accuracy = 0.594
I0812 10:49:41.753613 16199 solver.cpp:343]     Test net output #1: loss = 0.624903 (* 1 = 0.624903 loss)
I0812 10:49:41.753639 16199 solver.cpp:214] Iteration 8000, loss = 0.497251
I0812 10:49:41.753654 16199 solver.cpp:229]     Train net output #0: loss = 0.497251 (* 1 = 0.497251 loss)
I0812 10:49:41.753665 16199 solver.cpp:486] Iteration 8000, lr = 0.0001
I0812 10:49:41.758924 16199 solver.cpp:214] Iteration 8500, loss = 0.492079
I0812 10:49:41.758962 16199 solver.cpp:229]     Train net output #0: loss = 0.492079 (* 1 = 0.492079 loss)
I0812 10:49:41.758973 16199 solver.cpp:486] Iteration 8500, lr = 0.0001
I0812 10:49:41.763967 16199 solver.cpp:294] Iteration 9000, Testing net (#0)
I0812 10:49:41.763983 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.764016 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.764026 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.765278 16199 solver.cpp:343]     Test net output #0: accuracy = 0.7
I0812 10:49:41.765300 16199 solver.cpp:343]     Test net output #1: loss = 0.604608 (* 1 = 0.604608 loss)
I0812 10:49:41.765326 16199 solver.cpp:214] Iteration 9000, loss = 0.487063
I0812 10:49:41.765341 16199 solver.cpp:229]     Train net output #0: loss = 0.487063 (* 1 = 0.487063 loss)
I0812 10:49:41.765352 16199 solver.cpp:486] Iteration 9000, lr = 0.0001
I0812 10:49:41.770417 16199 solver.cpp:214] Iteration 9500, loss = 0.482196
I0812 10:49:41.770457 16199 solver.cpp:229]     Train net output #0: loss = 0.482196 (* 1 = 0.482196 loss)
I0812 10:49:41.770467 16199 solver.cpp:486] Iteration 9500, lr = 0.0001
I0812 10:49:41.775704 16199 net.cpp:763] Serializing 3 layers
I0812 10:49:41.775756 16199 solver.cpp:361] Snapshotting to /home/abba/caffe/examples/simple/simple_iter_10000.caffemodel
I0812 10:49:41.775934 16199 solver.cpp:369] Snapshotting solver state to /home/abba/caffe/examples/simple/simple_iter_10000.solverstate
I0812 10:49:41.776084 16199 solver.cpp:276] Iteration 10000, loss = 0.477472
I0812 10:49:41.776104 16199 solver.cpp:294] Iteration 10000, Testing net (#0)
I0812 10:49:41.776113 16199 net.cpp:671] Copying source layer data
I0812 10:49:41.776123 16199 net.cpp:671] Copying source layer fc1
I0812 10:49:41.776131 16199 net.cpp:671] Copying source layer loss
I0812 10:49:41.777367 16199 solver.cpp:343]     Test net output #0: accuracy = 0.74
I0812 10:49:41.777395 16199 solver.cpp:343]     Test net output #1: loss = 0.584324 (* 1 = 0.584324 loss)
I0812 10:49:41.777407 16199 solver.cpp:281] Optimization Done.
I0812 10:49:41.777416 16199 caffe.cpp:134] Optimization Done.
